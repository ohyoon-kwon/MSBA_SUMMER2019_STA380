#####Green Buildings###########################################################################

For the green building problem, we wanted to explore and calculate how much money we can save on energy and utility bills. Because green building should be energy efficient and we will save more money (= profit, basically)

```{r}

library(mosaic)
library(readr)
library(tidyverse)
green = read_csv("greenbuildings.csv")
head(green)

```
```{r}
cluster<- green %>% group_by(cluster)
cluster
```
We have ordered the data by their cluster, and we analyze the hot days & cold days total to figure out the demand for energy distribution across the clusters.

```{r}
summary(green$hd_total07)
summary(green$cd_total_07)
```
We decide to only exam the clusters that have similar weather to Austin, so our conclusion would be more relatable and convincing. We take out the data with "net=1" because residents pay their own utility bills and as building owners won't save much.

```{r}

green_zero = green %>% filter(net == 0 ) 
green_one = green %>% filter(net == 1 )

a = data.frame(group ="cd",value= green_zero$cd_total_07)
b = data.frame(group = "hd", value= green_zero$hd_total07)
plot.data = rbind(a, b) 
plot.data %>% group_by(group)
ggplot(plot.data, aes(x=group, y=value, fill=group)) +
 geom_boxplot()

f = data.frame(group = "non_net", value = nrow(green_zero))
d = data.frame(group = 'net', value = nrow(green_one))
c = rbind(f,d)
ggplot(c, aes(x=group, y=value, fill = group))+
  geom_col()
```
To define the "Austin weather", we take heatdays > median and colddays <median. Next we only gona work with cluster that matches this condition.

```{r}

one_q = quantile(green_zero$cd_total_07, .5)
one_q
three_q= quantile(green_zero$hd_total07, .5)

green_filter = green_zero  %>% 
  filter(green_zero$cd_total_07 < one_q)

green_filt = green_filter %>%
  filter(green_filter$hd_total07 > three_q)

print(green_filt)
green_filt%>% group_by(cluster)%>%summarize(count = n())
```

Then we tried the exact same analysis done by the immature "data scientist" did in the problem, just on our selected dataset.
```{r}
df = green_filt %>% filter(green_filt$leasing_rate > 10)
df_green= df %>% filter(green_rating == 1)
df_non_green= df %>% filter(green_rating != 1)

summary(df_green$Rent)
summary(df_non_green$Rent)
```

```{r}
green = median(df_green$Rent)
non_green = median(df_non_green$Rent)
new_rent_dif = green- non_green
amt_saved = new_rent_dif * 250000
amt_saved

```
We found out that in clusters that have similar weathcer as Austin, the rent median difference isn't that great bewteen green and non-green buildings. So there is a way smaller "extra revenue" we gonna make and so far ($27,500, comparing to 650,000 computed by the guy), it seems we will recuperate the green building costs in way longer than the 7 years time period he calculated. So bad deal?

We continue to explore the revenue generated by energy savings.
```{r}
df_green
df_green$gas = df_green$cd_total_07 * df_green$Gas_Costs
df_green$elec = df_green$hd_total07 * df_green$Electricity_Costs

df_green$total_saved = (df_green$gas + df_green$elec) * 0.25
df_green$total_cost = (df_green$gas + df_green$elec) * 0.75
df_green
```
Here, we made some assumptions about the dataset:

cold days means demand for gas (heating),
hot days means demand for electricity (cooling A/C)

So our formulas are:

cd_total  * Gas_Costs = gas bill per year
hd_total * Electricity_Costs = electricity bill per year

These cost are per unit area per year.

We also assumed that green buildings in general can save 25% energy compare to non-green,
according to numbers from LEED and National Geographics websites. So we multiple our cost by 0.25, and count it as the cost saved (extra revenue)

```{r}
hist(df_green$total_saved, 10)
median_saved = median(df_green$total_saved)
yearly_saved = median_saved * 250000
yearly_saved
```
We calculated the median of total_saved in clusters that have similar weather in Austin, then multiplied it by our building's planned area in the problem, 250,000.

Our yearly saving on utility bills would be around 9,235,600.


```{r}
green_mediancost = median(df_green$total_cost)
green_mean = mean(df_green$total_cost)

df_non_green$gas = df_non_green$cd_total_07 * df_non_green$Gas_Costs
df_non_green$elec = df_non_green$hd_total07 * df_non_green$Electricity_Costs
df_non_green$total_cost = (df_non_green$gas + df_non_green$elec)

nongreen_median_cost = median(df_non_green$total_cost)
nongreen_mean = mean(df_non_green$total_cost)

b = data.frame(group = 'Green Buildings', value = green_mean)
a = data.frame(group = 'Nongreen Buildings', value = nongreen_mean)
viz = rbind(a, b)
viz %>% group_by(group)

ggplot(viz, aes(x=group, y=value, fill=group)) +
  geom_col() + ylab('Cost of Gas+Electricity per year') + xlab("Building Type")

```
hd/cdboth counts bp
```{r}

e = green_zero %>%
  filter(green_zero$hd_total07 > three_q)

r = green_zero  %>% 
  filter(green_zero$cd_total_07 < one_q)

c= green_filt

g= data.frame(group= "above hd", value = nrow(e))
d = data.frame(group= "below cd", value = nrow(r))
c = data.frame(group = "total", value = nrow(c))

viz = rbind(g,d,c)
viz %>% group_by(group)

ggplot(viz, aes(x=group, y=value, fill=group)) +
 geom_col() + ylab("demand count")

```

Because yearly extra saving(revenue) is around 9 million per year, the green building 5% premium fee 5 million extra cost will be recuperated in less than a year. On top of that, the building owner would save a lot more each year on the utility bills for the building running in general.

So based on our approach, we conclude that green building is a great idea. The immature data scientist's approach has the similar conclusion as ours, but he ignored too many factors.

Our approach explored deeper into the factor of weather, and utility cost.

#Please clear your Environment before running the next problem. May have repeated variable names#
#####Flight at ABIA###########################################################################

```{r}
library(mosaic)
library(tidyverse)
library(ggplot2)
library(ggthemes)
library(ggpubr)
```

```{r}
flight = read.csv('ABIA.csv')
attach(flight)
names(flight)
```

We are interested in looking at the cancellation rate for each airport thus we created a subset with all cancellations. 

```{r}
cancel = subset(flight, Cancelled == 1)
cancel
```



```{r}
table1 = xtabs(~Cancelled + CancellationCode, data = cancel)

a1 = cancel %>%
  group_by(CancellationCode) %>%
  summarize(count = sum(Cancelled))
a1

ggbarplot(a1, x = 'CancellationCode', y = 'count',
   fill = 'CancellationCode', color = 'CancellationCode', palette = "jco", label = round(a1$count)) 
  
# ggplot(data = a1) +
#   geom_bar(mapping = aes(x=CancellationCode, y=count), stat='identity')
```

There are a totle of 1420 cancellations. 719 of them are type A cancellations which is cancellations related to the unique carriers,. 605 of them are cancellations due to wheater (Type B), and 96 of them are NAS(Type C) which are delays or cancellations within the control of National Airspace System. 


```{r}
b1 = cancel %>%
  group_by(DayOfWeek) %>%
  summarize(total = sum(Cancelled))
b1

ggbarplot(b1, x = 'DayOfWeek', y = 'total',
   fill ='DayOfWeek', color = 'DayOfWeek', label = round(b1$total))
```
Next we plotted cancellations by days of week and found that cancellations mostly happen on Tuesdays. 


```{r}
e1 = cancel %>%
  group_by(Origin) %>%
  summarize(count = sum(Cancelled))

subset(e1, Origin == 'AUS')

```


```{r}
c1 = cancel %>%
  group_by(Origin, CancellationCode) %>%
  summarize(count = sum(Cancelled))
c1
```

```{r}
Austin = subset(c1, Origin == 'AUS')
Austin
ggdotchart(Austin, x = "CancellationCode", y = "count",
           color = "CancellationCode", add = 'segment', dot.size = 6, label = round(Austin$count),
           rotate = TRUE, title = 'Austin Airport Flight Cancellations')    

```

Since we are looking at datasets in the Austin Airport. We are also interested in looking at the amount of flights that got cancelled departing from the  Austin Airport. The result shows that Austin Airport has a total of 732 cancellations and the most common cancellation type is Type A which is controlled by carrier. 


```{r}
a = sort(table(flight$Origin), decreasing = TRUE)
b =  sort(table(cancel$Origin), decreasing = TRUE)
```

```{r}
all_air = data.frame(a)
cancel_air = data.frame(b)
merged = merge(all_air, cancel_air, by = "Var1")
merged$percentdec = merged$Freq.y/merged$Freq.x

merged$percent = (merged$percentdec)*100


merged1 = subset(merged, Freq.y != 0)
merged1
```

```{r}
merged1 = merged1[order(merged1$percent),]
merged1$Var1=factor(merged1$Var1,levels=merged1$Var1)
ggplot(data = merged1) +
  geom_bar(mapping = aes(x=factor(Var1), y=percent), stat='identity', ascending=FALSE) +
  coord_flip()
```

Finally, we are interested in looking at how Austin Airport's cancellations is compare to other airports. We normalized the number of cancellations into percentage of cancellation out of all flights in each airport, and we graphed the result. In turns out that St. Louis Airport is most frequent in canceling its flights with 6.32% cancelled flights out of all flights. Austin is actually ranked number 12th with 1.48% cancelled flights out of all flights.

#Please clear your Environment before running the next problem. May have repeated variable names#
#####Portfolio Modeling########################################################################

Please also see the report for more details of the portfolio, in (.pdf) format attached in the same folder!

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(mosaic)
library(quantmod)
library(foreach)
```

###Develop 3 different portfolios

# First one: "Invest in China" ! not diversified, focus on 3 equities ETF and 1 money with RMB
#CQQQ: Invesco China Technology ETF
#CYB: WisdomTree Chinese Yuan Fund
#EWH:iShares MSCI Hong Kong ETF 
#CXSE: WisdomTree China ex-State-Owned Enterprises Fund
---------------------------------------------------------------------
###Extract data and adjust
```{r}
# Import a few stocks
myETF = c("CQQQ", "CYB", "EWH", "CXSE")
myprices = getSymbols(myETF, from = "2014-08-13") #five years ago
# A chunk of code for adjusting all stocks
for(ticker in myETF) {
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
	eval(parse(text=expr))
}
# Combine all the returns in a matrix
all_returns = cbind(	ClCl(CQQQa),
								ClCl(CYBa),
								ClCl(EWHa),
								ClCl(CXSEa))
all_returns = as.matrix(na.omit(all_returns))
head(all_returns)
# Compute the returns from the closing prices
```
###Doing the simulation
```{r}
# Run simulation 5000 times 
initial_wealth = 100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
	total_wealth = initial_wealth
	n_days = 20
	wealthtracker = rep(0, n_days) #initialize a list of 0
	for(today in 1:n_days) {
	  weights = c(0.25, 0.25, 0.25, 0.25) #4 ETF in total
		holdings = weights * total_wealth
		
		return.today = resample(all_returns, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		wealthtracker[today] = total_wealth
	}
	wealthtracker
}
# Profit/loss
hist(sim1[,n_days]- initial_wealth, breaks=30, main = "Invest In China Portfolio GAIN&LOSS", xlab = "US dollar", col = "red")
```
###Compute VaR with 5% cut.
```{r}
quantile((sim1[,n_days]- initial_wealth), probs = c(0.05, 0.95))
```

-------------------------------------------------------------------------------------------------
#Portfolio Number 2:

VNG: Vanguard Real Estate Index Fund
USO: United States Oil Fund
SHV: iShares Short Treasury Bond ETF
VCSH: Vanguard Short-Term Corporate Bond ETF
JPST: JPMorgan Ultra-Short Income ETF
SCZ: iShares MSCI EAFE Small-Cap ETF
XLV: Health Care Select Sector SPDR Fund
GLD: SPDR Gold Trust

run similar simulation as previous portfolio
```{r}
myETF = c("VNQ", "USO", "SHV", "VCSH","JPST", "SCZ", "XLV", "GLD")
myprices = getSymbols(myETF, from = "2014-08-13")
for(ticker in myETF) {
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
	eval(parse(text=expr))
}
all_returns = cbind(	ClCl(VNQ),
								ClCl(USO),
								ClCl(SHV),
								ClCl(VCSH),
								ClCl(JPST),
								ClCl(SCZ),
								ClCl(XLV),
								ClCl(GLD))
all_returns = as.matrix(na.omit(all_returns))
head(all_returns)
```
```{r}
initial_wealth = 100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
	total_wealth = initial_wealth
	n_days = 20
	wealthtracker = rep(0, n_days) 
	for(today in 1:n_days) {
	  weights = c(0.125, 0.125, 0.125, 0.125,0.125, 0.125, 0.125, 0.125) #8 ETF total
		holdings = weights * total_wealth
		return.today = resample(all_returns, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		wealthtracker[today] = total_wealth
	}
	wealthtracker
}
# Profit/loss
hist(sim1[,n_days]- initial_wealth, breaks=30, main = "Diversified Portfolio GAIN&LOSS", xlab = "US dollar", col = "blue")
```
###Compute VaR with 5% cut.
```{r}
quantile((sim1[,n_days]- initial_wealth), probs = c(0.05, 0.95))
```
-------------------------------------------------------------------------------------------------
#Portfolio Number 3:
XWEB: SPDR S&P Internet ETF 
IBUY: Amplify Online Retail ETF
JSMD: Janus Henderson Small/Md Cp Gr Alpha ETF
IFLY: ETFMG Drone Economy Strategy ETF

```{r}
myETF = c("XWEB", "IBUY", "JSMD", "IFLY")
myprices = getSymbols(myETF, from = "2014-08-13")
for(ticker in myETF) {
	expr = paste0(ticker, "a = adjustOHLC(", ticker, ")")
	eval(parse(text=expr))
}
all_returns = cbind(	ClCl(XWEB),
								ClCl(IBUY),
								ClCl(JSMD),
								ClCl(IFLY))
all_returns = as.matrix(na.omit(all_returns))

initial_wealth = 100000
sim1 = foreach(i=1:5000, .combine='rbind') %do% {
	total_wealth = initial_wealth
	n_days = 20
	wealthtracker = rep(0, n_days) 
	for(today in 1:n_days) {
	  weights = c(0.25, 0.25, 0.25, 0.25)
		holdings = weights * total_wealth
		return.today = resample(all_returns, 1, orig.ids=FALSE)
		holdings = holdings + holdings*return.today
		total_wealth = sum(holdings)
		wealthtracker[today] = total_wealth
	}
	wealthtracker
}
# Profit/loss
hist(sim1[,n_days]- initial_wealth, breaks=30, main = "Invest In Online/Growing Industries", xlab = "US dollar", col = "green")
```
```{r}
quantile((sim1[,n_days]- initial_wealth), probs = c(0.05, 0.95))
```

#Please clear your Environment before running the next problem. May have repeated variable names#
#####Market Segmentation#######################################################################

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(foreach)
library(mvtnorm)
library(LICORS)
library(readr)
library(ggplot2)
library(ggthemes)
social = read.csv("social_marketing.csv", row.names=1)
```

```{r}
sm_clean = select(social, -matches("uncategorized"))
sm_cleaned = subset(sm_clean, adult < 5)
```
The Median number of adult tags is 5, so we decide to take out data points containing more than 5 adult tweets per week. also we found out for spam, it ranges from 0-2 with mean around 0.3, so we conclude that the filter has done a pretty good job already. We not gonna touch the spam column or drop anything. Becasue uncategorized tags don't help in segmentation, we drop that whole column. 

```{r}
set.seed(1)
k_grid = seq(2, 20, by=1)
SSE_grid = foreach(k = k_grid, .combine='c') %do% {
  cluster_k = kmeans(sm_cleaned, k, nstart=50)
  cluster_k$tot.withinss
}
plot(k_grid, SSE_grid)
```
After seeing the results from k-means, we decide we will pick k = 7

```{r}
k = 7
cluster_k = kmeans(sm_cleaned, k, nstart=50)
```
K-means clustering with 7 clusters of sizes 917, 583, 475, 3533, 544, 376, 1164

```{r}
for(i in 1:k) { 
  index_temp = which(cluster_k$cluster == i)
  df_temp = sm_cleaned[c(index_temp), ]
  assign(paste("ddat",i,sep="_"),df_temp) 
  assign(paste("index",i,sep="_"),index_temp) 
} 
```
Now we have 7 different df for each cluster

###A quick try on Hie-clustering
```{r}
sm_distance_matrix = dist(social, method='euclidean')
hmin = hclust(sm_distance_matrix, method='complete')
cluster3 = cutree(hmin, k=10)
summary(factor(cluster3))
```
We have tried method "complete" and "single", also k = 5-10. The results have been disappointing because there's always one big group that takes over majority of data points. We are showing the best one we've got. So we decide to stick with results from K-means for now.

-------------------------------------------------------------------------------------------------
###PCA
We need to label each data with a group number (7 different segmentations!) for later 
The group assignments are based on results from k means
```{r}
sm_cleaned['group'] = NA

sm_cleaned$group[index_1] = 'group1'
sm_cleaned$group[index_2] = 'group2'
sm_cleaned$group[index_3] = 'group3'
sm_cleaned$group[index_4] = 'group4'
sm_cleaned$group[index_5] = 'group5'
sm_cleaned$group[index_6] = 'group6'
sm_cleaned$group[index_7] = 'group7'
```

```{r}
z = sm_cleaned[,1:35]
pc1 = prcomp(z, scale.=TRUE)
```

```{r}
loadings = pc1$rotation
scores = pc1$x
qplot(scores[,1], scores[,2], color = sm_cleaned$group, xlab = 'Component 1', ylab = 'Component 2') +scale_color_colorblind()
```

It's obvious that there are some distinct groups such as 3, 5 and 6.
Let's do some head & tail to analyze what component 1 & 2 represents.
```{r}
o1 = order(loadings[,1], decreasing = TRUE)
colnames(z)[head(o1,10)]
colnames(z)[tail(o1,10)]
colnames(z)[o1[20:25]]

o2 = order(loadings[,2], decreasing = TRUE)
colnames(z)[head(o2,10)]
colnames(z)[tail(o2,10)]
colnames(z)[o2[20:25]]
```

Seperate them into individual plots to see lcearly
```{r}
sm.group = sm_cleaned$group

qplot(scores[,1], scores[,2], facets=~sm.group, xlab = 'Component 1', ylab = 'Component 2')
```
Group3 is closer to head of PC1 and PC2 than any other groups.
"religion"      "food"          "parenting"     "sports_fandom" "school"       
"family"        "beauty"        "crafts"        "cooking"       "fashion"
 
"religion"      "sports_fandom" "parenting"     "food"          "school"       
"family"        "news"          "automotive"    "adult"         "crafts"

We conclude that this group should be more family oriented, older adults who's already married. 
-----------------------------------------------------------------------------------------------
Group6 is close to PC1 tail and PC2 middle:
 "dating"           "current_events"  "art"              "tv_film"          "college_uni"      "online_gaming"   "adult"            "spam" 
"computers"      "travel"         "tv_film"        "dating"         "current_events"
"online_gaming"

We conclude this group is close to college students, young single adults.
---------------------------------------------------------------------------------------------
Group 5 is close to tail of PC 2:
   "health_nutrition" "personal_fitness" "music"          "chatter"          "beauty"           "shopping"         "fashion"      "cooking"          "photo_sharing" 
   
We conclude this group as working woman, or our "mum" group
--------------------------------------------------------------------------------------------
Other clusters that were kinda in the middles, pretty similar:

They do contain some unique features that stood out, such as "business","small business", and "news" that were not as common as other groups. So we identify these groups as various working class, hard workers, or entrepreneur. 




#Please clear your Environment before running the next problem. May have repeated variable names#
#####Author Attribution########################################################################
```{r}
library(tm) 
library(magrittr)
library(dplyr)
library(slam)
library(proxy)
library(stringr)
library(tidyverse)


readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), 
            id=fname, language='en') }
```
###EXTRACTING THE DATA###

```{r}
a_fold= list.dirs(path = "C:/Users/Cleme/Documents/GitHub/STA380/data/ReutersC50/C50train", full.names = TRUE, recursive = TRUE)
a_fold = as.list(a_fold)
a_fold = a_fold[-1]


str2 = '/*.txt'
str1  = 'C:/Users/Cleme/Documents/GitHub/STA380/data/ReutersC50/C50train/'
for (i in seq_along(a_fold)) {
  tempstring = strsplit((as.character(a_fold[i])),"/" )
  tempstring = tempstring[[1]][10]
  stringtemp = paste(str1, tempstring,str2, sep = '')
  temp = Sys.glob(stringtemp)
  assign(paste("txtfile", i , sep= "_"), temp)
}
#extracting all the files from each folder in the training set

```


### cleaning all the documents inside each author folder and creating a DTM for each document ###

stopwords were removed and a TFIDF matrix was created and then converted to a dataframe
to be used in a classification model.


```{r}

for (j in 1:50){

x = eval(parse(text=paste("txtfile", j , sep= "_")))

aaron = lapply(x, readerPlain)
mynames = x %>%
{ strsplit(., '/', fixed=TRUE) } %>%
{ lapply(., tail, n=2) } %>%
{ lapply(., paste0, collapse = '') } %>%
  unlist
 names(aaron) = mynames

documents_raw = Corpus(VectorSource(aaron))
my_documents = documents_raw
my_documents = tm_map(my_documents, content_transformer(tolower)) # make everything lowercase
my_documents = tm_map(my_documents, content_transformer(removeNumbers)) # remove numbers
my_documents = tm_map(my_documents, content_transformer(removePunctuation)) # remove punctuation
my_documents = tm_map(my_documents, content_transformer(stripWhitespace)) ## remove excess white-space

my_documents = tm_map(my_documents, content_transformer(removeWords), stopwords("en"))
DTM_temp = DocumentTermMatrix(my_documents)
DTM_temp = removeSparseTerms(DTM_temp, 0.95)

tfidf_temp = weightTfIdf(DTM_temp)
X = as.matrix(tfidf_temp)

df = as.data.frame(X)
assign(paste("df", j , sep= "_"), df)
}

#cleaning the words and then calculating the TFIDF for each term in the documents for the authors
#assigning this into a dataframe

```

```{r}
library(tidyverse)
library(data.table)

name_list = list()

for (i in seq(1:50)) {
  x_df = eval(parse(text=paste("df", i , sep= "_")))

  
  tempstring = strsplit((as.character(a_fold[i])),"/" )
  tempstring = tempstring[[1]][10]

  setDF(x_df)[]
  y_df = x_df

  y_df[,"author_name"] = tempstring

  name_list[[i]] = y_df
  assign(paste("df", i , "t", sep= "_"), y_df)
  
}

```
```{r}
library(plyr)

a_w_list = rbind.fill(name_list)

a_w_list = select(a_w_list, -cusersclemedocumentsgithubstadatareuterscctrainalancrosbynewsmltxt)
#cleaning up wrong extraction
a_w_list[is.na(a_w_list)] <- 0


```
###TESTSET###

```{r}

a_fold2= list.dirs(path = "C:/Users/Cleme/Documents/GitHub/STA380/data/ReutersC50/C50test", full.names = TRUE, recursive = TRUE)
a_fold2 = as.list(a_fold2)
a_fold2 = a_fold2[-1]
str2 = '/*.txt'
str1a  = 'C:/Users/Cleme/Documents/GitHub/STA380/data/ReutersC50/C50test/'
for (i in seq_along(a_fold)) {
  tempstring = strsplit((as.character(a_fold2[i])),"/" )
  tempstring = tempstring[[1]][10]
  stringtemp = paste(str1a, tempstring,str2, sep = '')
  temp = Sys.glob(stringtemp)
  assign(paste("t_file", i , sep= "_"), temp)
}

for (j in 1:50){

x = eval(parse(text=paste("t_file", j , sep= "_")))

aaron = lapply(x, readerPlain)
mynames = x %>%
{ strsplit(., '/', fixed=TRUE) } %>%
{ lapply(., tail, n=2) } %>%
{ lapply(., paste0, collapse = '') } %>%
  unlist
 names(aaron) = mynames

documents_raw = Corpus(VectorSource(aaron))
my_documents = documents_raw
my_documents = tm_map(my_documents, content_transformer(tolower)) # make everything lowercase
my_documents = tm_map(my_documents, content_transformer(removeNumbers)) # remove numbers
my_documents = tm_map(my_documents, content_transformer(removePunctuation)) # remove punctuation
my_documents = tm_map(my_documents, content_transformer(stripWhitespace)) ## remove excess white-space

my_documents = tm_map(my_documents, content_transformer(removeWords), stopwords("en"))
DTM_temp = DocumentTermMatrix(my_documents)
DTM_temp = removeSparseTerms(DTM_temp, 0.95)

tfidf_temp = weightTfIdf(DTM_temp)
X = as.matrix(tfidf_temp)


df = as.data.frame(X)
assign(paste("df_test", j , sep= "_"), df)
}


name_list2 = list()

for (i in seq(1:50)) {
  x_df = eval(parse(text=paste("df_test", i , sep= "_")))

  
  tempstring = strsplit((as.character(a_fold[i])),"/" )
  tempstring = tempstring[[1]][10]

  setDF(x_df)[]
  y_df = x_df
  y_df[,"author_name"] = tempstring
  name_list2[[i]] = y_df
  assign(paste("df_test", i , "t", sep= "_"), y_df)
  
}

a_w_listtest = rbind.fill(name_list2)
a_w_listtest[is.na(a_w_listtest)] <- 0
a_w_listtest = select(a_w_listtest, -cusersclemedocumentsgithubstadatareuterscctestaaronpressmannewsmltxt)

```


###KNN WITH COSINE DISTANCE###
```{r}

library(class)

a_w_listtrain = select(a_w_list, -author_name)
a_w_listtest2 = select(a_w_listtest, -author_name)
#creating pseudo words
for (i in seq(1:59)){
pseud_count <- 1
a_w_listtest2[ , paste0("psued_word", i)] <-pseud_count
#pseudoword for all discrepancy between test/train
}
dim(a_w_listtest2)
dim(a_w_listtrain)


cosine_dist_mat = proxy::dist(as.matrix(a_w_listtrain), method='cosine')
cosine_dist_mat2 = proxy::dist(as.matrix(a_w_listtest2), method='cosine')
#creating cosine distance matrix for training and testing 

model1<- knn(cosine_dist_mat, cosine_dist_mat2, a_w_list$author_name, k=70)
mean(a_w_list$author_name== model1)

#KNN run on the cosine distance matrices; returns an accuracy of around 2%

```




#Please clear your Environment before running the next problem. May have repeated variable names#
#####Association Rule Mining###################################################################

```{r}
library(tidyverse)
library(arules)  # has a big ecosystem of packages built around it
library(arulesViz)
```


Read in the groceries transactions
This is in "long" format -- every row is a basket with multiple items per row separated by commas. We separated the items in each basket by commas
```{r}
groceries_raw = read.transactions("groceries.txt", sep=',')
```

Checking if our items are separated correctly
```{r}
inspect(head(groceries_raw, 2))
LIST(head(groceries_raw, 3))
```

Checking if our items are separated correctly
```{r}
inspect(head(groceries_raw, 2))
LIST(head(groceries_raw, 3))
```

```{r}
str(groceries_raw)
summary(groceries_raw)
```

Counting the frequencies of each of the grocery items and plotting the top 20 items that appear most frequently.
```{r}
frequent = eclat(groceries_raw, parameter = list(supp = 0.07, maxlen = 15))
inspect(frequent)
itemFrequencyPlot(groceries_raw, topN=20, type="absolute", main="Item Frequency")
```

Creating a list of baskets: vectors of items by consumer
Analagous to bags of words

Cast this variable as a special arules "transactions" class.
```{r}
grotrans = as(groceries_raw, "transactions")
summary(grotrans)
```

Running the 'apriori' algorithm
Looking at rules with support > .005 & confidence >.1 & length (# artists) <= 5
```{r}
shoppingrules = apriori(grotrans, 
                     parameter=list(support=.005, confidence=.1, maxlen=5))
```

Looking at the output
```{r}
#inspect(shoppingrules)
```


Choosing a subset
For our thresholds for lift and confidence, we chose confidence to be greater than 0.5 and lift to be greater than 3.
When we initially inspected when lift > 4, only four connections were returned. These returns had very low confidence ranging from 0.12 to
0.44, although the lifts were high. This means that these connections should not be correct most of the time.
When we inspected for confidence > 0.6, 22 connections were returned. These returns had lifts that were below 3, which means that there is
a greater chance of the connections being a coincidence compared to when lift is greater than 3.
Thus, when we inspected for our threshold, 8 connections that were high in both confidence and lift were returned. These connections are
therefore not just a confidence and they are correct most of the time.

Our choice of threshold is also supported by the Gephi diagram (using Force Atlas). The grocery items that have the most connections are found
from the connections returned by applying the threshold.

The results from the table returned and the gephi diagram(uploaded as another file on github) makes sense. For example, it is reasonable to claim that customers purchasing onions and root vegetables also purchase other vegetables. Also, it makes sense for customers purchasing curd and tropical fruits to purchase yogurt (a combination of ingredients people usually have).
```{r}
inspect(subset(shoppingrules, subset=lift > 4))
inspect(subset(shoppingrules, subset=confidence > 0.6))
inspect(subset(shoppingrules, subset=lift > 3 & confidence > 0.5))
```


ploting all the rules in the (support, confidence) space
Higher lift rules tend to have lower support
```{r}
plot(shoppingrules)
```

Swapping the axes and color scales
```{r}
plot(shoppingrules, measure = c("support", "lift"), shading = "confidence")
```


"two key" plot: coloring is by size (order) of item set
```{r}
plot(shoppingrules, method='two-key plot')
```

looking at subsets driven by the plot
```{r}
inspect(subset(shoppingrules, support > 0.035))
inspect(subset(shoppingrules, confidence > 0.5 & lift > 3))
```

graph-based visualization
For the visualization, we used the threshold that we chose earlier (confidence > 0.5 & lift > 3). After plotting the subset, we are able
to clearly see strong connections between the grocery items. We tried using a confidence that was higher, which did not show as many
interesting and insightful connections.

```{r}
sub1 = subset(shoppingrules, subset=confidence > 0.5 & lift > 3)
summary(sub1)
plot(sub1, method='graph')
?plot.rules
```


```{r}
plot(head(sub1, 100, by='lift'), method='graph')
```

export
#There is a file on github showing the connections after applying 'Force Alas' called 'Gephi_ForceAtlas.png'
```{r}
saveAsGraph(head(shoppingrules, n = 1000, by = "lift"), file = "shoppingrules.graphml")
```

